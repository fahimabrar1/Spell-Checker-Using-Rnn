# -*- coding: utf-8 -*-
"""EDRNN_Sen_New.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b9tFJuflvwTdp5xd6qPGE7r9owfAxaso
"""

# !pip install bltk

import os
from collections import Counter

import numpy as np
import pandas as pd
import tensorflow as tf
from bltk.langtools import Tokenizer as blT
from bltk.langtools.banglachars import (vowels,
                                        vowel_signs,
                                        consonants,
                                        digits,
                                        others)
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
# from google.colab import drive
# drive.mount('/content/drive')

nukta = u'\u09bc'

tndset_c = pd.read_csv('E:\Programming\Python\SP\Data\_final_wr_word_corpus_25.csv')
tndset_c = tndset_c.dropna()
print(tndset_c)

# wdf = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/final_all_words_corpus.csv')
# words = list(wdf['words'])

# tndset = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/tndset_sen - Sheet1.csv')
tndset = pd.read_csv('E:\Programming\Python\SP\Data\_final_wr_sentence_corpus_25.csv')
tndset = tndset.dropna()
print(tndset)

# Test Train Split

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(tndset['wrong'], tndset['correct'], test_size=0.1)

print("X_train Length: " , len(X_train))
print("X_train" , X_train)
print("New Line")
print()

print("X_test Length: " , len(X_test))
print("X_test" , X_test)
print("New Line")
print()

print("y_train Length: " , len(y_train))
print("y_train" , y_train)
print("New Line")
print()

print("y_test Length: " , len(y_test))
print("y_test" , y_test)

new_trn_tdnset = pd.DataFrame(list(zip(X_train, y_train)), columns =['wrong', 'correct'])
new_tst_tdnset = pd.DataFrame(list(zip(X_test,y_test)), columns =['wrong', 'correct'])


starter = "<start>"
ender = "<end>"


def read_corpus(corpus):
    fc = []
    tokenizer = blT()
    words = []
    for i, sen in enumerate(corpus['correct']):
        word = tokenizer.word_tokenizer(sen)
        for j in word:
            print("j", j)
            words.append(j)
    for i, sen in enumerate(corpus['wrong']):
        word = tokenizer.word_tokenizer(sen)
        for j in word:
            print("j", j)
            words.append(j)
    return words


words = read_corpus(tndset)

# allw_df = pd.DataFrame(words, columns=["words"]) 
# allw_df = allw_df.reset_index(drop=True)
# allw_df.to_csv('/content/drive/MyDrive/Colab Notebooks/final_all_words_corpus.csv')

len(words)

vocabs = set(words)
vocabs

len(vocabs)

word_counts = Counter(words)
word_counts

wordVocav = []
wordVocav.append(starter)
wordVocav.append(ender)
for i in vocabs:
    wordVocav.append(i)

print("Word Vocab Length: " + str(len(wordVocav)))
print(wordVocav)
print(len(wordVocav))

charVocab = []
charVocab.append(nukta)
for i in vowels:
    charVocab.append(i)

for i in vowel_signs:
    charVocab.append(i)

for i in consonants:
    charVocab.append(i)

for i in digits:
    charVocab.append(i)

for i in range(0, 5):
    charVocab.append(others[i])
print("Char Vocab Length: " + str(len(charVocab)))
print(charVocab)
print(len(charVocab))

w_vocabdict = dict(zip(wordVocav, range(0, len(wordVocav))))
w_vocabdict

rw_vocabdict = dict(zip(range(0, len(wordVocav)), wordVocav))
rw_vocabdict

c_vocabdict = dict(zip(charVocab, range(1, len(charVocab) + 1)))
c_vocabdict

rc_vocabdict = dict(zip(range(1, len(charVocab) + 1), charVocab))
rc_vocabdict

#train Set
trnX = new_trn_tdnset
trnX = trnX.reset_index(drop=True)

# Test Set
tstX = new_tst_tdnset
tstX = tstX.reset_index(drop=True)


tX_c = tndset_c
tX_c

c_newdf = tX_c
c_newdf

cnt = 0
for i in c_newdf['wrong']:
    if type(i) == float:
        cnt += 1
        print(i)
print(cnt)

w_trn_newdf = trnX
w_tst_newdf = tstX


t_steps = 0
for i in w_trn_newdf['wrong']:
    word_list = i.split()
    if t_steps < len(word_list):
        t_steps = len(word_list) + 1
print(t_steps)

maxlen = max(c_newdf['wrong'].values, key=len)
c_t_steps = len(maxlen) + 1
print(c_t_steps)

from keras import layers
from numpy import argmax
from numpy import array

for i in range(0):
    print(i)


def one_hot_encode_char(sequence, n_unique):
    encoding = []
    emp = []
    count = 0
    for i, letter in enumerate(sequence):
        vector = [0 for _ in range(n_unique)]
        # print(c_vocabdict[letter])
        vector[c_vocabdict[letter]] = 1
        encoding.append(vector)
        count += 1
    for i in range(c_t_steps - (count)):
        vector = [0 for _ in range(n_unique)]
        vector[0] = 1
        emp.append(vector)
    encoding = emp + encoding
    x = array(encoding)
    x = x.reshape((1, x.shape[0], x.shape[1]))
    return x


# one hot encode sequence
def one_hot_encode(sequence, n_unique):
    encoding = []
    emp = []
    count = 0
    w = sequence.split()
    for i, letter in enumerate(w):
        vector = [0 for _ in range(n_unique)]
        print(w_vocabdict[letter])
        vector[w_vocabdict[letter]] = 1
        encoding.append(vector)
        count += 1
    for i in range(t_steps - (count)):
        vector = [0 for _ in range(n_unique)]
        vector[0] = 1
        emp.append(vector)
    encoding = emp + encoding
    # print(sequence)
    # print(w)
    # print(encoding)
    x = array(encoding)
    x = x.reshape((1, x.shape[0], x.shape[1]))
    return x


# decode a one hot encoded string
def one_hot_decode(encoded_seq):
    ls = []
    for vector in encoded_seq:
        ls.append(argmax(vector))
    return ls


# arrX = []
# arrY = []
count = 0

trainX = [one_hot_encode(i[0], len(wordVocav)) for i in trnX.values]
testX = [one_hot_encode(i[0], len(wordVocav)) for i in tstX.values]
trainY = [one_hot_encode(i[1], len(wordVocav)) for i in trnX.values]
testY = [one_hot_encode(i[1], len(wordVocav)) for i in tstX.values]

trainY[0]

count = 0

for i, c in enumerate(trainX):
    if len(c[0]) > 10:
        count += 1
print(count)

count = 0
for i in trainX:
    if len(i[0]) > 10:
        count += 1

print(count)

count = 0
for i in trainY:
    if len(i[0]) > 15:
        count += 1

print(count)

count = 0
lsX = list()
for i in trainX:
    lsX.append(i)

lsY = list()
for i in trainY:
    lsY.append(i)

print(len(wordVocav))

trainY = np.array(trainY)
trainY.shape
testY = np.array(testY)
trainX = np.array(trainX)
testX = np.array(testX)
trainY[:2]
print(trainX.shape)
print(trainY.shape)

type(trainX)

trainX = np.array(trainX).squeeze()
trainY = np.array(trainY).squeeze()
testX = np.array(testX).squeeze()
testY = np.array(testY).squeeze()

print(trainX[0])

numberofLSTMUnits = 64  # @param {type:"integer"}
batch_size = 8  # @param {type:"integer"}
n_features = len(wordVocav)

batch_size = 1

from keras.layers import Lambda
from keras import backend as K

test_input_data = np.zeros((batch_size, 1, n_features))
print(test_input_data[0])
test_input_data[:, 0, 0] = 1
print(test_input_data[0])
print(test_input_data)

print(trainX.shape)
print(trainY.shape)

newX = trainX
newY = trainY


def create_model():
    encoder_inputs = layers.Input(shape=(t_steps, n_features), name='encoder_inputs')

    # encoder_lstm1 = layers.LSTM(numberofLSTMUnits, return_state=True,  name='encoder_lstm')
    # encoder_outputs1, state_h1, state_c1 = encoder_lstm(encoder_inputs)
    # states1 = [state_h1, state_c1]

    encoder_lstm = layers.LSTM(numberofLSTMUnits, return_state=True, name='encoder_lstm1')
    encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)

    # initial context vector is the states of the encoder
    states = [state_h, state_c]

    # Set up the decoder layers
    # Attention: decoder receives 1 token at a time &
    # decoder outputs 1 token at a time
    decoder_inputs = layers.Input(shape=(1, n_features))
    decoder_lstm = layers.LSTM(numberofLSTMUnits, return_sequences=True,
                               return_state=True, name='decoder_lstm')
    decoder_dense = layers.Dense(n_features, activation='softmax', name='decoder_dense')

    all_outputs = []
    # Prepare decoder initial input data: just contains the START character 0
    # Note that we made it a constant one-hot-encoded in the model
    # that is, [1 0 0 0 0 0 0 0 0 0] is the initial input for each loop
    decoder_input_data = np.zeros((batch_size, 1, n_features))
    decoder_input_data[:, 0, 0] = 1

    # that is, [1 0 0 0 0 0 0 0 0 0] is the initial input for each loop
    inputs = decoder_input_data
    # decoder will only process one time step at a time
    # loops for fixed number of time steps: n_timesteps_in
    for _ in range(t_steps):
        # Run the decoder on one time step
        outputs, state_h, state_c = decoder_lstm(inputs,
                                                 initial_state=states)
        outputs = decoder_dense(outputs)
        # Store the current prediction (we will concatenate all predictions later)
        all_outputs.append(outputs)
        # Reinject the outputs as inputs for the next loop iteration
        # as well as update the states
        inputs = outputs
        states = [state_h, state_c]

    # Concatenate all predictions such as [batch_size, timesteps, features]
    decoder_outputs = Lambda(lambda x: K.concatenate(x, axis=1))(all_outputs)

    # Define and compile model
    return tf.keras.Model([encoder_inputs], decoder_outputs, name='model_encoder_decoder')


model = create_model()
# plot_model(model, show_shapes=True,show_layer_names=True)

# model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])
# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss='categorical_crossentropy',
#               metrics=['accuracy'])
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4), loss='categorical_crossentropy',
              metrics=['acc'])

model.summary()

checkpoint_path = "E:\Programming\Python\SP\Checkpoints\_rnn_cp-{epoch:04d}.ckpt"
# checkpoint_path = "E:\Programming\Python\SP\Checkpoints\_rnn_cp-0020.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)

# Create a callback that saves the model's weights
cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                 save_weights_only=True,
                                                 verbose=1)
"""Load Weights"""

# latest_cp = tf.train.latest_checkpoint(checkpoint_dir)
# latest_cp = 'E:\Programming\Python\SP\Checkpoints\_bkp\_rnn_cp-0010.ckpt'
# print(latest_cp)
# model.load_weights(latest_cp)

len(lsX)

len(lsY)

print(trainX.shape)
print(trainY.shape)

print(newX.shape)
print(newY.shape)

model.fit(newX,
          newY,
          batch_size=batch_size,
          epochs=25,
          callbacks=[cp_callback]
          )
model.save('E:\Programming\Python\SP\Checkpoints\model\my_model')
"""Save Weights"""

model.save_weights(checkpoint_path.format(epoch=0))

# n = newX
#
# n = n.reshape(newX.shape[0], 1, newX.shape[1], newX.shape[2])
n = testX

n = n.reshape(testX.shape[0], 1, testX.shape[1], testX.shape[2])

print(len(n))
n.shape

correctGuessCounter = 0
correctToCount = len(n)
y_pred = []
for j in range(len(n)):
    # print(j)
    wrong = n[j]
    pred_Wrong = model.predict(wrong)
    ls = [[argmax(i)] for i in pred_Wrong[0]]
    testls = []
    for i in ls:
        if i[0] != 0:
            testls.append(rw_vocabdict[i[0]])
            # print(testls)

    p = " ".join(testls)
    y_pred.append(p)
    # print("wrong Word:",end='')
    L = tstX.iloc[j]['wrong']

    print(L,end="\n")

    print("Predicted Word:",end='')
    print(p,end="\n")

    print("Correct Word:",end='')
    L = tstX.iloc[j]['correct']
    print(L,end="\n")

    if p == L:
        correctGuessCounter += 1
        print("L")

print("Accuracy:" + str(correctGuessCounter))
print("Accuracy: ", str((correctGuessCounter / correctToCount)))
# print("y_pred: ", y_pred)
# print("y_test: ", y_test)
#
# print('Precision: %.3f' % precision_score(len(y_test.values.tolist()), len(y_pred)))
# print('Recall: %.3f' % recall_score(len(y_test.values.tolist()), len(y_pred)))
# print('Accuracy: %.3f' % accuracy_score(len(y_test.values.tolist()), len(y_pred)))
# print('F1 Score: %.3f' % f1_score(len(y_test.values.tolist()), len(y_pred)))